/**
 * 测试思路
 * 1. 先用 100 万数据集测试
 * 2. 再用 1亿数据 验证结果, 使用 ClickHouse 计算正确结果
 */

/**
 * 算法思路
 * 100GB 数据/假设一条记录 192 Bytes
 * 100 * 1024 * 1024 * 1024 / 200 = 536870912
 * 大约 5 亿多条数据
 * 1GB 内存能装大概 5368709 条数据
 * 分治
 * 1. 让一样的 url 都分配到同一个文件
 * 2. 让我们考虑一种情况，假如某一个 hash 过后的文件超过 5368709 条记录，那么我们的内存是装不下的
 *     2.1 先尝试使用 HashMap 统计
 *     2.2 如果超额则再分治
 * 归并
 * 1. 对每个文件求 topK 数据(topK 数据由最大堆维护)
 * 2. 把 X 个文件的 topK 再归并一次
 */

/**
 * 优化思路
 * 1. 如果文件个数少于模数，那么一定有大量重复，直接用 HashMap 即可
 */

/**
 * 复杂度分析
 * 分治一次 O(N) + 如果数据倾斜仍然有一次分治 O(N)
 * 对每个文件求 topK O(N) * log(K)
 * 再归并 X * K * log(100) = 7*X*K << N
 * 最坏情况需要 (2+log(K)) * O(N)
 * IO 次数
 * 1. 分治 IO 一次 + 如果数据倾斜仍然有一次分治 IO
 * 2. 对每个文件求 topK
 */

/**
 * 扩展性
 * 1. 如果数据分布不确定怎么办 ? 考虑两个极端 1. 所有数据都是一样的 2. 所有数据都不一样
 * 2. 如果要任意 top 数怎么办 ? 
 */
const auto RECORD_BYTES = 20; // assume 20 bytes for every url


